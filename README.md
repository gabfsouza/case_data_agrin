# Case Data Agrin - Pipeline de Dados Bronze-Silver-Gold

## ğŸ“– Sobre o Projeto

Este projeto implementa uma arquitetura de datalake moderna seguindo o padrÃ£o **Medallion Architecture** (Bronze-Silver-Gold), utilizando Apache Spark para processamento de dados, MinIO como armazenamento S3-compatÃ­vel e Prefect como orquestrador de workflows.

## ğŸ—ï¸ Arquitetura

### A Jornada dos Dados

O pipeline processa dados de e-commerce atravÃ©s de trÃªs camadas distintas:

1. **Bronze Layer** (`bronze/`)
   - **Origem**: API FakeStore (produtos, categorias, carrinhos)
   - **FunÃ§Ã£o**: IngestÃ£o bruta dos dados, preservando o formato original
   - **Destino**: MinIO bucket `datalake/bronze/`
   - **Formato**: JSON particionado por data (ano/mÃªs/dia)

2. **Silver Layer** (`silver/`)
   - **Origem**: Dados brutos do Bronze
   - **FunÃ§Ã£o**: Limpeza, validaÃ§Ã£o e enriquecimento dos dados
   - **Processamento**: Spark SQL para transformaÃ§Ãµes e normalizaÃ§Ã£o
   - **Destino**: MinIO bucket `datalake/silver/`
   - **Formato**: Parquet otimizado para consultas

3. **Gold Layer** (`gold/`)
   - **Origem**: Dados processados do Silver
   - **FunÃ§Ã£o**: AgregaÃ§Ãµes e modelagem de indicadores de negÃ³cio
   - **Processamento**: CÃ¡lculo de mÃ©tricas e KPIs
   - **Destino**: SQLite local (`gold/gold.db`)
   - **Uso**: AnÃ¡lises e relatÃ³rios finais

### OrquestraÃ§Ã£o com Prefect

O **Prefect** atua como o maestro da orquestraÃ§Ã£o, garantindo que cada etapa seja executada na ordem correta:

- **Fluxo Sequencial**: Bronze â†’ Silver â†’ Gold
- **Retry AutomÃ¡tico**: Em caso de falha, tenta novamente automaticamente
- **Agendamento**: ExecuÃ§Ã£o diÃ¡ria Ã s 6h (horÃ¡rio de BrasÃ­lia)
- **Monitoramento**: Interface web para acompanhamento em tempo real

## ğŸš€ Como Executar

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- Python 3.8+ (para executar o orquestrador)

### Passo a Passo

1. **Subir a infraestrutura**:
   ```bash
   docker-compose up -d
   ```

2. **Aguardar inicializaÃ§Ã£o** (cerca de 30-60 segundos):
   - MinIO serÃ¡ configurado automaticamente com bucket `datalake` e pastas `bronze`, `silver`, `gold`
   - Spark container estarÃ¡ pronto para processar jobs

3. **Iniciar o orquestrador Prefect**:
   ```bash
   python orchestrator/apollo_flow_prefect.py
   ```
   
   âš ï¸ **Importante**: Este comando deve ser executado localmente (nÃ£o dentro do container) e manterÃ¡ o processo rodando para que a DAG apareÃ§a na interface do Prefect.

### Acessos

ApÃ³s subir os serviÃ§os, vocÃª pode acessar:

- **MinIO Console**: http://localhost:9001
  - UsuÃ¡rio: `minioadmin`
  - Senha: `minioadmin123`
  - API S3: http://localhost:9000

- **Prefect UI**: http://localhost:4200
  - Interface para monitorar e executar workflows

- **Spark UI**: http://localhost:4040
  - Monitoramento de jobs Spark em execuÃ§Ã£o

## ğŸ“ Estrutura do Projeto

```
case_data_agrin/
â”œâ”€â”€ bronze/              # Camada Bronze - IngestÃ£o de dados
â”‚   â”œâ”€â”€ insert_s3.py     # Job principal de ingestÃ£o
â”‚   â””â”€â”€ fakestore_api.py # Cliente da API FakeStore
â”œâ”€â”€ silver/              # Camada Silver - Processamento e limpeza
â”‚   â”œâ”€â”€ services/        # Processadores de dados
â”‚   â””â”€â”€ adapters/        # Adaptadores para Spark
â”œâ”€â”€ gold/                # Camada Gold - AgregaÃ§Ãµes e indicadores
â”‚   â”œâ”€â”€ modeling_indicators.py
â”‚   â””â”€â”€ gold.db          # Banco SQLite com resultados
â”œâ”€â”€ orchestrator/        # OrquestraÃ§Ã£o Prefect
â”‚   â””â”€â”€ apollo_flow_prefect.py
â”œâ”€â”€ infra/               # ConfiguraÃ§Ãµes de infraestrutura
â”‚   â”œâ”€â”€ Dockerfile       # Imagem Spark customizada
â”‚   â””â”€â”€ requirements.txt # DependÃªncias Python
â””â”€â”€ docker-compose.yml   # OrquestraÃ§Ã£o de containers
```

## ğŸ”§ Tecnologias

- **Apache Spark**: Processamento distribuÃ­do de dados
- **MinIO**: Armazenamento S3-compatÃ­vel
- **Prefect**: OrquestraÃ§Ã£o de workflows
- **SQLite**: Banco de dados para camada Gold
- **Python**: Linguagem principal do projeto

## ğŸ“ Notas

- O pipeline Ã© **idempotente**: pode ser executado mÃºltiplas vezes sem duplicar dados
- Os dados sÃ£o particionados por data para otimizar consultas
- O container Spark monta o workspace como volume para desenvolvimento facilitado

## ğŸ“Š VisualizaÃ§Ã£o dos Dados

ApÃ³s a execuÃ§Ã£o do pipeline, os dados processados na camada Gold ficam disponÃ­veis no banco SQLite localizado em `gold/gold.db`.

### Conectando com DBeaver

Recomendamos o uso do **DBeaver** para visualizar e consultar os dados:

1. **Instalar DBeaver**: Baixe em https://dbeaver.io/download/

2. **Criar nova conexÃ£o**:
   - Tipo: SQLite
   - Caminho do banco: `gold/gold.db` (caminho absoluto do arquivo no seu projeto)

3. **Explorar os dados**: ApÃ³s conectar, vocÃª poderÃ¡ visualizar todas as tabelas e indicadores gerados pela camada Gold, executar queries SQL e exportar resultados.

